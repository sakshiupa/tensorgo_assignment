!pip install -U openai-whisper
!pip install sentence_transformers
! pip install gradio==3.50.2
import whisper
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration
from sentence_transformers import SentenceTransformer, util
from transformers import MarianMTModel, MarianTokenizer, pipeline
import torch


model_name = "facebook/rag-sequence-nq"
tokenizer = RagTokenizer.from_pretrained(model_name)
model = RagSequenceForGeneration.from_pretrained(model_name)

# Import the Whisper multilingual model.

whisperModel = whisper.load_model("base")

#Import Sentence Transformer in order to extract

# Specify a document structure akin to RAG.

documents = {
        "doc1": {"title": "Artificial Intelligence and Machine Learning", "content": "This document describes the fundamentals of artificial intelligence and machine learning."},
    "doc2": {"title": "Deep Learning and Neural Networks", "content": "This document explores the concepts of deep learning and neural networks."},
    "doc3": {"title": "Impact of AI on Different Industries", "content": "This document discusses the impact of AI on various sectors like healthcare, finance, and more."},
    "doc4": {"title": "Future of Technology", "content": "This document highlights advancements in AI, quantum computing, and other fields shaping the future of technology."},
    "doc5": {"title": "The Promise of AI", "content": [
        "The future of AI holds immense potential, marked by rapid progress and transformative capabilities across various industries.",
        "AI is expected to revolutionize healthcare, finance, transportation, and education by enabling more efficient and accurate decision-making processes.",
        "In healthcare, AI can support early diagnosis, personalized treatment plans, and even robotic surgeries.",
        "The finance sector can leverage AI-driven algorithms for fraud detection and optimal trading strategies.",
        "The transportation sector is likely to see widespread adoption of autonomous vehicles, improving safety and reducing congestion.",
        "Education can benefit from personalized learning experiences tailored to individual student needs."
    ]}

}

# Condense the content of the document for encoding.

allDocumentTexts = []
for key, value in documents.items():
    if isinstance(value, list):
        allDocumentTexts.extend(value)
    else:
        allDocumentTexts.append(value)

# Use the retriever model to encrypt the documents.

documentEmbeddings = retrieverModel.encode(allDocumentTexts, convert_to_tensor=True)

def transcribeSpeech(filePath):
    """Transcribes speech using Multilingual Whisper"""
    result = whisperModel.transcribe(filePath)
    return result["text"]

def detectLanguage(audioPath):
    """Detects the language of the audio using Multilingual Whisper"""
    audio = whisper.load_audio(audioPath)
    audio = whisper.pad_or_trim
    _, probs = whisperModel.detect_language(mel)
    detectedLanguageCode = max(probs, key=probs.get)

    languageMapping = {
        'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',
        'hi': 'Hindi', 'ja': 'Japanese', 'ru': 'Russian', 'ar': 'Arabic',
        'te': 'Telugu', 'zh': 'Chinese', 'pt': 'Portuguese'
    }

    return languageMapping.get(detectedLanguageCode, detectedLanguageCode).capitalize()

def loadTranslationModel(sourceLanguage, targetLanguage):
    """Loads translation model and tokenizer based on languages"""
    modelName = {
        ("English", "Hindi"): "Helsinki-NLP/opus-mt-en-hi",
        ("English", "Spanish"): "Helsinki-NLP/opus-mt-en-es",
        ("English", "Japanese"): "Helsinki-NLP/opus-mt-en-jap",
        ("English", "German"): "Helsinki-NLP/opus-mt-en-de",
        ("English", "Russian"): "Helsinki-NLP/opus-mt-en-ru",
        ("English", "Arabic"): "Helsinki-NLP/opus-mt-en-ar",
        ("English", "Telugu"): "Helsinki-NLP/opus-mt-en-te",
        ("English", "French"): "Helsinki-NLP/opus-mt-en-fr",
        ("English", "Italian"): "Helsinki-NLP/opus-mt-en-it",
        ("English", "Chinese"): "Helsinki-NLP/opus-mt-xx-zh",
        ("Hindi", "English"): "Helsinki-NLP/opus-mt-hi-en",
        ("Spanish", "English"): "Helsinki-NLP/opus-mt-es-en",
        ("Japanese", "English"): "Helsinki-NLP/opus-mt-jap-en",
        ("German", "English"): "Helsinki-NLP/opus-mt-de-en",
        ("Russian", "English"): "Helsinki-NLP/opus-mt-ru-en",
        ("Arabic", "English"): "Helsinki-NLP/opus-mt-ar-en",
        ("Telugu", "English"): "Helsinki-NLP/opus-mt-te-en",
        ("French", "English"): "Helsinki-NLP/opus-mt-fr-en",
        ("Italian", "English"): "Helsinki-NLP/opus-mt-it-en",
        ("Chinese", "English"): "Helsinki-NLP/opus-mt-zh-en"
    }

    if (sourceLanguage, targetLanguage) not in modelName:
        raise ValueError(f"Translation model for {sourceLanguage} to {targetLanguage} not available.")

    translationModel = MarianMTModel.from_pretrained(modelName[(sourceLanguage, targetLanguage)])

def translateText(text, model, tokenizer):
    """Translates text using the specified model and tokenizer"""
    inputs = tokenizer(text, return_tensors="pt", padding=True)
    with torch.no_grad():
        translatedTokens = model.generate(**inputs)
    translation = tokenizer.decode(translatedTokens[0], skip_special_tokens=True)
    return translation

def retrieveDocument(query):
    """Retrieves a document based on the query"""
    queryEmbedding = retrieverModel.encode(query, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(queryEmbedding, documentEmbeddings)[0]
    topScoreIdx = scores.argmax().item()
    return list(documents.values())[topScoreIdx]
# Pipeline for load summarization

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarizeText(text):
    """Summarizes the text"""
    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
    return summary[0]['summary_text']

def processAudio(audio, targetLanguage):
    """Processes the audio file and returns transcriptions, translations, and summaries"""
    transcription = transcribeSpeech(audio)
    detectedLanguage = detectLanguage(audio)

# If the transcription isn't already in English, translate it there.

    if detectedLanguage != "English":
        translationModel, translationTokenizer = loadTranslationModel(detectedLanguage, "English")
        transcriptionInEnglish = translateText(transcription, translationModel, translationTokenizer)
    else:
        transcriptionInEnglish = transcription

# If the target language is not English, translate the English transcription to the other language.

    if targetLanguage != "English":
        translationModel, translationTokenizer = loadTranslationModel("English", targetLanguage)
        translatedText = translateText(transcriptionInEnglish, translationModel, translationTokenizer)
    else:
        translatedText = transcriptionInEnglish

# Get the document using the text transcription as a guide.

    retrievedDocument = retrieveDocument(transcriptionInEnglish)
    summarizedText = summarizeText(transcriptionInEnglish)
    return transcription, detectedLanguage, translatedText, retrievedDocument, summarizedText

# Develop the interface for Gradio.

iface = gr.Interface(
    fn=processAudio,
    inputs=[
        gr.Audio(source="upload", type="filepath"),
        gr.Dropdown(["Hindi", "Spanish", "Japanese", "German", "Russian", "Arabic", "French", "Italian", "Chinese", "English"], label="Target Language")
    ],
    outputs=[
        gr.Textbox(label="Transcription"),
        gr.Textbox(label="Detected Language"),
        gr.Textbox(label="Translation"),
        gr.Textbox(label="Retrieved Document"),
        gr.Textbox(label="Summarized Text")
    ],
    title="Multilingual Speech Recognition, Translation, Document Retrieval, and Summarization",
    description="Upload an audio file in any language, select a target language to get the transcription, translation, retrieve a document based on the transcription, and get a summary of the transcription."
)

# Start the Gradio user interface.

iface.launch()
